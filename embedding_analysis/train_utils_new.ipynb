{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,logging\n",
    "import numpy as np\n",
    "%run corpus_parser_new.ipynb\n",
    "%run utils.ipynb\n",
    "%run skipgram_new_backup.ipynb\n",
    "\n",
    "global_subgraph_id_freq_map_as_list = []\n",
    "\n",
    "#from corpus_parser import Corpus\n",
    "#from utils import save_graph_embeddings\n",
    "#from skipgram import skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set it manually for now\n",
    "# corpus_dir = \"/home/ipsita/BTP/graph2vec/data/kdd_datasets/ptc\"\n",
    "# output_dir = \"/home/ipsita/BTP/graph2vec/embeddings\"\n",
    "# batch_size = 128\n",
    "# epochs = 1000\n",
    "# embedding_size = 1024\n",
    "# num_negsample = 10\n",
    "# learning_rate = 0.3\n",
    "# wlk_h = 3\n",
    "# label_filed_name = 'Label'\n",
    "# class_labels_fname = '/home/ipsita/BTP/graph2vec/data/kdd_datasets/ptc.Labels'\n",
    "\n",
    "# extn = 'g2v'+str(wlk_h)    # wlk_h is height to be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ipsita/BTP/graph2vec/embeddings/ptc_dims_1024_epochs_1000_lr_0.3_embeddings.txt\n"
     ]
    }
   ],
   "source": [
    "# op_fname = '_'.join([os.path.basename(corpus_dir), 'dims', str(embedding_size), 'epochs',\n",
    "#                          str(epochs),'lr',str(learning_rate),'embeddings.txt'])\n",
    "\n",
    "# op_fname = os.path.join(output_dir, op_fname)\n",
    "\n",
    "# if os.path.isfile(op_fname):\n",
    "#     logging.info('The embedding file: {} is already present, hence NOT training skipgram model '\n",
    "#                  'for subgraph vectors'.format(op_fname))\n",
    "# print(op_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing SKIPGRAM...\n",
      "INFO:root:number of graphs: 344\n",
      "INFO:root:subgraph vocabulary size: 3804\n",
      "INFO:root:total number of subgraphs to be trained: 34837\n"
     ]
    }
   ],
   "source": [
    "# logging.info(\"Initializing SKIPGRAM...\")\n",
    "# corpus = Corpus(corpus_dir, extn = extn, max_files=0)  # just load 'max_files' files from this folder\n",
    "# corpus.scan_and_load_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_skipgram = skipgram(\n",
    "#         num_graphs=corpus.num_graphs,\n",
    "#         num_subgraphs=corpus.num_subgraphs,\n",
    "#         learning_rate=learning_rate,\n",
    "#         embedding_size=embedding_size,\n",
    "#         num_negsample=num_negsample,\n",
    "#         num_steps=epochs,  # no. of time the training set will be iterated through\n",
    "#         corpus=corpus,  # data set of (target,context) tuples\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_skipgram (corpus_dir, extn, learning_rate, win_size, concat_flag, cpath, X, embedding_size, num_negsample, epochs, batch_size, output_dir):\n",
    "\n",
    "    op_fname = '_'.join([os.path.basename(corpus_dir), 'dims', str(embedding_size), 'epochs',\n",
    "                         str(epochs),'lr',str(learning_rate),'embeddings_test.txt'])\n",
    "    op_fname = os.path.join(output_dir, op_fname)\n",
    "    if os.path.isfile(op_fname):\n",
    "        logging.info('The embedding file: {} is already present, hence NOT training skipgram model '\n",
    "                     'for subgraph vectors'.format(op_fname))\n",
    "        return op_fname\n",
    "\n",
    "    logging.info(\"Initializing TEST SKIPGRAM...\")\n",
    "    corpus = Corpus(win_size, corpus_folder = corpus_dir, extn = extn, max_files=0)  # just load 'max_files' files from this folder\n",
    "    corpus.scan_and_load_corpus_for_test(X)\n",
    "\n",
    "    model_skipgram = skipgram_test(\n",
    "        num_graphs=corpus.num_graphs,\n",
    "        learning_rate=learning_rate,\n",
    "        win_size = win_size,\n",
    "        concat_flag = concat_flag,\n",
    "        cpath = cpath,\n",
    "        batch_size = batch_size,\n",
    "        embedding_size=embedding_size,\n",
    "        num_negsample=num_negsample,\n",
    "        num_steps=epochs,  # no. of time the training set will be iterated through\n",
    "        corpus=corpus,  # data set of (target,context) tuples\n",
    "    )\n",
    "\n",
    "    final_embeddings = model_skipgram.infer(global_subgraph_id_freq_map_as_list, corpus=corpus,batch_size=batch_size)\n",
    "\n",
    "    logging.info('Write the matrix to a word2vec format file')\n",
    "    save_graph_embeddings(corpus, final_embeddings, op_fname)\n",
    "    logging.info('Completed writing the final embeddings, pls check file: {} for the same'.format(op_fname))\n",
    "    return op_fname\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram (corpus_dir, extn, learning_rate, win_size, concat_flag, cpath, X, embedding_size, num_negsample, epochs, batch_size, output_dir):\n",
    "    '''\n",
    "\n",
    "    :param corpus_dir: folder containing WL kernel relabeled files. All the files in this folder will be relabled\n",
    "    according to WL relabeling strategy and the format of each line in these folders shall be: <target> <context 1> <context 2>....\n",
    "    :param extn: Extension of the WL relabled file\n",
    "    :param learning_rate: learning rate for the skipgram model (will involve a linear decay)\n",
    "    :param embedding_size: number of dimensions to be used for learning subgraph representations\n",
    "    :param num_negsample: number of negative samples to be used by the skipgram model\n",
    "    :param epochs: number of iterations the dataset is traversed by the skipgram model\n",
    "    :param batch_size: size of each batch for the skipgram model\n",
    "    :param output_dir: the folder where embedding file will be stored\n",
    "    :return: name of the file that contains the subgraph embeddings (in word2vec format proposed by Mikolov et al (2013))\n",
    "    '''\n",
    "    global  global_subgraph_id_freq_map_as_list\n",
    "    op_fname = '_'.join([os.path.basename(corpus_dir), 'dims', str(embedding_size), 'epochs',\n",
    "                         str(epochs),'lr',str(learning_rate),'embeddings.txt'])\n",
    "    op_fname = os.path.join(output_dir, op_fname)\n",
    "    if os.path.isfile(op_fname):\n",
    "        logging.info('The embedding file: {} is already present, hence NOT training skipgram model '\n",
    "                     'for subgraph vectors'.format(op_fname))\n",
    "        return op_fname\n",
    "    \n",
    "    op_fname_sub = '_'.join([os.path.basename(corpus_dir), 'dims', str(embedding_size), 'epochs',\n",
    "                            str(epochs), 'lr', str(learning_rate), 'subgraph_embedding.txt'])\n",
    "    op_fname_sub = os.path.join(output_dir, op_fname_sub)\n",
    "    if os.path.isfile(op_fname_sub):\n",
    "        logging.info('The subgraph embedding file: {} is already present, hence NOT training skipgram model '\n",
    "                     'for subgraph vectors'.format(op_fname))\n",
    "        return op_fname_sub\n",
    "\n",
    "    \n",
    "    logging.info(\"Initializing SKIPGRAM...\")\n",
    "    corpus = Corpus(win_size, corpus_folder = corpus_dir, extn = extn, max_files=0)  # just load 'max_files' files from this folder\n",
    "    global_subgraph_id_freq_map_as_list = []\n",
    "    global_subgraph_id_freq_map_as_list = corpus.scan_and_load_corpus(X)\n",
    "    \n",
    "    with open('/home/ipsita/BTP/graph2vec/tmp/for_gui/subgraph_id2freq.p', 'wb') as f:\n",
    "        pickle.dump(global_subgraph_id_freq_map_as_list, f)\n",
    "        \n",
    "    model_skipgram = skipgram(\n",
    "        num_graphs=corpus.num_graphs,\n",
    "        num_subgraphs=corpus.num_subgraphs,\n",
    "        learning_rate=learning_rate,\n",
    "        win_size = win_size,\n",
    "        concat_flag = concat_flag,\n",
    "        cpath = cpath,\n",
    "        batch_size = batch_size,\n",
    "        embedding_size=embedding_size,\n",
    "        num_negsample=num_negsample,\n",
    "        num_steps=epochs,  # no. of time the training set will be iterated through\n",
    "        corpus=corpus,  # data set of (target,context) tuples\n",
    "    )\n",
    "\n",
    "    final_embeddings, subgraph_embeddings = model_skipgram.train(corpus=corpus,batch_size=batch_size)\n",
    "\n",
    "    logging.info('Write the matrix to a word2vec format file')\n",
    "    save_graph_embeddings(corpus, final_embeddings, op_fname)\n",
    "    logging.info('Completed writing the final embeddings, pls check file: {} for the same'.format(op_fname))\n",
    "    \n",
    "    save_subgraph_embeddings(corpus, subgraph_embeddings, op_fname_sub)\n",
    "    logging.info('Completed writing the final subgraph embeddings, pls check file: {} for the same'.format(op_fname_sub))\n",
    "\n",
    "    return op_fname, op_fname_sub\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
