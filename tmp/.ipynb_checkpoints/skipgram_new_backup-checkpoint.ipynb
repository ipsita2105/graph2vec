{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math,logging\n",
    "from pprint import  pprint\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipgram(object):\n",
    "    '''\n",
    "    skipgram model - refer Mikolov et al (2013)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_graphs, num_subgraphs, learning_rate, win_size, concat_flag, cpath, batch_size, embedding_size, num_negsample, num_steps, corpus):\n",
    "        \n",
    "        self.num_graphs     = num_graphs\n",
    "        self.num_subgraphs  = num_subgraphs\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_negsample  = num_negsample\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.win_size       = win_size\n",
    "        self.concat_flag    = concat_flag\n",
    "        self.cpath          = cpath\n",
    "        self.batch_size     = batch_size\n",
    "        self.num_steps      = num_steps\n",
    "        self.corpus         = corpus\n",
    "        self.graph, self.batch_inputs, self.batch_labels, self.normalized_embeddings, self.loss, self.optimizer = self.trainer_initial()\n",
    "\n",
    "    def trainer_initial(self):\n",
    "        \n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            \n",
    "            wsize = self.win_size\n",
    "            batch_inputs = tf.placeholder(tf.int32, shape=([None, 2*wsize+1]))\n",
    "            batch_labels = tf.placeholder(tf.int64, shape=([None, 1]))\n",
    "\n",
    "            graph_embeddings = tf.Variable(\n",
    "                    tf.random_uniform([self.num_graphs, self.embedding_size], -0.5 / self.embedding_size, 0.5/self.embedding_size)\n",
    "                    ,name=\"graph_embeddings\")\n",
    "\n",
    "            subgraph_embeddings = tf.Variable(\n",
    "                    tf.random_uniform([self.num_subgraphs, self.embedding_size], -0.5/self.embedding_size, 0.5/self.embedding_size)\n",
    "                    ,name=\"subgraph_embeddings\")\n",
    "            \n",
    "            embed = []\n",
    "            batch_doc_embedding = tf.nn.embedding_lookup(graph_embeddings, batch_inputs[:,0])\n",
    "            if self.concat_flag is 1:\n",
    "                for j in range(1, 2*wsize+1):\n",
    "                    embed_w = tf.nn.embedding_lookup(subgraph_embeddings, batch_inputs[:,j])\n",
    "                    embed.append(embed_w)\n",
    "                weights = tf.Variable(tf.truncated_normal([self.num_subgraphs, (2*wsize+1)*(self.embedding_size)], stddev=1.0 / math.sqrt((2*wsize+1)*(self.embedding_size)))\n",
    "                                     ,name=\"weights\")\n",
    "            else:\n",
    "                embed_w = tf.zeros([self.batch_size, self.embedding_size])            \n",
    "                for j in range(1, 2*wsize+1):\n",
    "                    embed_w += tf.nn.embedding_lookup(subgraph_embeddings, batch_inputs[:,j])\n",
    "                embed.append(embed_w)\n",
    "                weights = tf.Variable(tf.truncated_normal([self.num_subgraphs, 2*self.embedding_size], stddev=1.0 / math.sqrt(2*self.embedding_size))\n",
    "                                     ,name=\"weights\")\n",
    "                \n",
    "            embed.append(batch_doc_embedding)\n",
    "            final_embed = tf.concat(embed, 1)\n",
    "            biases = tf.Variable(tf.zeros(self.num_subgraphs), name=\"bias\")\n",
    "            \n",
    "            #negative sampling part\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.nce_loss(weights = weights,\n",
    "                               biases  = biases,\n",
    "                               labels  = batch_labels,\n",
    "                               inputs   = final_embed,\n",
    "                               num_sampled = self.num_negsample,\n",
    "                               num_classes = self.num_subgraphs,\n",
    "                               sampled_values = tf.nn.fixed_unigram_candidate_sampler(\n",
    "                                   true_classes = batch_labels,\n",
    "                                   num_true = 1,\n",
    "                                   num_sampled = self.num_negsample,\n",
    "                                   unique = True,\n",
    "                                   range_max = self.num_subgraphs,\n",
    "                                   distortion = 0.75,\n",
    "                                   unigrams = self.corpus.subgraph_id_freq_map_as_list)#word_id_freq_map_as_list is the\n",
    "                               # frequency of each word in vocabulary\n",
    "                               ))\n",
    "\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            learning_rate = tf.train.exponential_decay(self.learning_rate,\n",
    "                                                       global_step, 100000, 0.96, staircase=True) #linear decay over time\n",
    "\n",
    "            learning_rate = tf.maximum(learning_rate,0.001) #cannot go below 0.001 to ensure at least a minimal learning\n",
    "\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "\n",
    "            norm = tf.sqrt(tf.reduce_mean(tf.square(graph_embeddings), 1, keep_dims=True))\n",
    "            normalized_embeddings = graph_embeddings/norm\n",
    "            \n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "        print(\"Shape final embed\",final_embed.get_shape().as_list())\n",
    "        return graph, batch_inputs, batch_labels, normalized_embeddings, loss, optimizer\n",
    "\n",
    "    def train(self,corpus,batch_size):\n",
    "\n",
    "        with tf.Session(graph=self.graph,\n",
    "                       config=tf.ConfigProto(log_device_placement=True,allow_soft_placement=False)) as sess:\n",
    "\n",
    "            #init = tf.global_variables_initializer()\n",
    "            sess.run(self.init)\n",
    "\n",
    "            loss = 0\n",
    "            loss_list = []\n",
    "            \n",
    "            for i in range(self.num_steps):\n",
    "                t0 = time.time()\n",
    "                step = 0\n",
    "                while corpus.epoch_flag == False:\n",
    "                    batch_data, batch_labels = corpus.generate_batch_from_file(batch_size)# get (target,context) word_id tuples\n",
    "\n",
    "                    feed_dict = {self.batch_inputs:batch_data,self.batch_labels:batch_labels}\n",
    "                    _,loss_val = sess.run([self.optimizer,self.loss],feed_dict=feed_dict)\n",
    "\n",
    "                    loss += loss_val\n",
    "\n",
    "                    if step % 100 == 0:\n",
    "                        if step > 0:\n",
    "                            average_loss = loss/step\n",
    "                            logging.info( 'Epoch: %d : Average loss for step: %d : %f'%(i,step,average_loss))\n",
    "                    step += 1\n",
    "\n",
    "                corpus.epoch_flag = False\n",
    "                epoch_time = time.time() - t0\n",
    "                logging.info('#########################   Epoch: %d :  %f, %.2f sec.  #####################' % (i, loss/step,epoch_time))\n",
    "                loss_list.append(loss/step)\n",
    "                loss = 0\n",
    "\n",
    "            #done with training\n",
    "            final_embeddings = self.normalized_embeddings.eval()\n",
    "            plt.plot(loss_list)\n",
    "            \n",
    "            #### Save model here inside this session ####\n",
    "            #save_path = self.saver.save(sess, \"/home/ipsita/BTP/graph2vec/model_ckpt/model.ckpt\")\n",
    "            save_path = self.saver.save(sess, self.cpath + \"/model\"+str(self.win_size)+\".ckpt\")\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "            sess.close()\n",
    "            \n",
    "        return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipgram_test(object):\n",
    "    '''\n",
    "    skipgram model - refer Mikolov et al (2013)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_graphs, learning_rate, win_size, concat_flag, cpath, batch_size, embedding_size, num_negsample, num_steps, corpus):\n",
    "        \n",
    "        self.num_graphs     = num_graphs\n",
    "        self.num_subgraphs  = 0\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_negsample  = num_negsample\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.win_size       = win_size\n",
    "        self.concat_flag    = concat_flag\n",
    "        self.cpath          = cpath\n",
    "        self.batch_size     = batch_size\n",
    "        self.num_steps      = num_steps\n",
    "        self.corpus         = corpus\n",
    "        #self.graph = self.trainer_initial()\n",
    "        \n",
    "    #     def trainer_initial(self):\n",
    "\n",
    "    #         graph = tf.Graph()\n",
    "    #         with graph.as_default():\n",
    "\n",
    "    #             graph_embeddings_test = tf.Variable(\n",
    "    #                     tf.random_uniform([self.num_graphs, self.embedding_size], -0.5 / self.embedding_size, 0.5/self.embedding_size))\n",
    "\n",
    "    #         return graph\n",
    "    \n",
    "    def infer(self, subgraph_id_freq_map_as_list, corpus, batch_size):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        #restore model \n",
    "        sess = tf.Session()\n",
    "        #saver = tf.train.import_meta_graph('/home/ipsita/BTP/graph2vec/model_ckpt/model.ckpt.meta')\n",
    "        #saver.restore(sess, save_path='/home/ipsita/BTP/graph2vec/model_ckpt/model.ckpt')\n",
    "       \n",
    "        print(\"path1\", self.cpath + '/model'+str(self.win_size)+'.ckpt.meta')\n",
    "        print(\"path2\", self.cpath + '/model'+str(self.win_size)+'.ckpt')\n",
    "        saver = tf.train.import_meta_graph(self.cpath + '/model'+str(self.win_size)+'.ckpt.meta')\n",
    "        ###############saver.restore(sess, save_path=self.cpath + '/model'+str(self.win_size)+'.ckpt')\n",
    "        \n",
    "        #print(\"Variables are -\")\n",
    "        #for v in tf.get_default_graph().as_graph_def().node:\n",
    "        #      print(v.name)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        \n",
    "        wsize = self.win_size\n",
    "        batch_inputs_test = tf.placeholder(tf.int32, shape=([None, 2*wsize+1]))\n",
    "        batch_labels_test = tf.placeholder(tf.int64, shape=([None, 1]))\n",
    "\n",
    "        print(\"Graph to infer =\", self.num_graphs)\n",
    "        \n",
    "        graph_embeddings_test = tf.Variable(\n",
    "            tf.random_uniform([self.num_graphs, self.embedding_size], -0.5 / self.embedding_size, 0.5/self.embedding_size)\n",
    "            ,name=\"graph_embeddings_test\")\n",
    "        \n",
    "        subgraph_embeddings_test = graph.get_tensor_by_name(\"subgraph_embeddings:0\")\n",
    "        print(\"subgraph embedding size\",subgraph_embeddings_test.get_shape().as_list())\n",
    "        self.num_subgraphs = subgraph_embeddings_test.get_shape().as_list()[0]  #### NOTE THIS \n",
    "        \n",
    "        embed = []\n",
    "        batch_doc_embedding_new = tf.nn.embedding_lookup(graph_embeddings_test, batch_inputs_test[:,0])\n",
    "        \n",
    "        if self.concat_flag is 1:\n",
    "            for j in range(1, 2*wsize+1):\n",
    "                embed_w = tf.nn.embedding_lookup(subgraph_embeddings_test, batch_inputs_test[:,j])\n",
    "                embed.append(embed_w)\n",
    "        else:\n",
    "            embed_w = tf.zeros([self.batch_size, self.embedding_size])            \n",
    "            for j in range(1, 2*wsize+1):\n",
    "                embed_w += tf.nn.embedding_lookup(subgraph_embeddings_test, batch_inputs_test[:,j])\n",
    "            embed.append(embed_w)\n",
    "\n",
    "        embed.append(batch_doc_embedding_new)\n",
    "        final_embed_new = tf.concat(embed, 1)\n",
    "        \n",
    "        weights_test = graph.get_tensor_by_name(\"weights:0\")\n",
    "        biases_test = graph.get_tensor_by_name(\"bias:0\")\n",
    "        \n",
    "        #negative sampling part\n",
    "        ## TODO check this function ##\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(weights = weights_test,\n",
    "                           biases  = biases_test,\n",
    "                           labels  = batch_labels_test,\n",
    "                           inputs   = final_embed_new,\n",
    "                           num_sampled = self.num_negsample,\n",
    "                           num_classes = self.num_subgraphs,\n",
    "                           sampled_values = tf.nn.fixed_unigram_candidate_sampler(\n",
    "                               true_classes = batch_labels_test,\n",
    "                               num_true = 1,\n",
    "                               num_sampled = self.num_negsample,\n",
    "                               unique = True,\n",
    "                               range_max = self.num_subgraphs,\n",
    "                               distortion = 0.75,\n",
    "                               unigrams = subgraph_id_freq_map_as_list)\n",
    "                           \n",
    "                           ))\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate,\n",
    "                                                   global_step, 100000, 0.96, staircase=True) #linear decay over time\n",
    "\n",
    "        learning_rate = tf.maximum(learning_rate, 0.001) #cannot go below 0.001 to ensure at least a minimal learning\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step, var_list=[graph_embeddings_test])\n",
    "\n",
    "        norm = tf.sqrt(tf.reduce_mean(tf.square(graph_embeddings_test), 1, keep_dims=True))\n",
    "        normalized_embeddings = graph_embeddings_test/norm\n",
    "\n",
    "        self.init = tf.global_variables_initializer() ######## Change here\n",
    "        sess.run(self.init)\n",
    "        saver.restore(sess, save_path=self.cpath + '/model'+str(self.win_size)+'.ckpt')\n",
    "        #sess.run()########################################\n",
    "        \n",
    "        #init_new = tf.initialize_variables([graph_embeddings_test])\n",
    "        #sess.run(init_new)\n",
    "        \n",
    "        loss_sum = 0\n",
    "        loss_list = []\n",
    "\n",
    "        for i in range(self.num_steps):\n",
    "            t0 = time.time()\n",
    "            step = 0\n",
    "            while corpus.epoch_flag == False:\n",
    "                batch_data, batch_labels = corpus.generate_batch_from_file_for_test(batch_size)# get (target,context) word_id tuples\n",
    "\n",
    "                feed_dict = {batch_inputs_test:batch_data, batch_labels_test:batch_labels}\n",
    "                _,loss_val = sess.run([optimizer, loss],feed_dict=feed_dict)\n",
    "\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                if step % 100 == 0:\n",
    "                    if step > 0:\n",
    "                        average_loss = loss_sum/step\n",
    "                        logging.info( 'Epoch: %d : Average loss for step: %d : %f'%(i,step,average_loss))\n",
    "                step += 1\n",
    "\n",
    "            corpus.epoch_flag = False\n",
    "            epoch_time = time.time() - t0\n",
    "            logging.info('#########################   Epoch: %d :  %f, %.2f sec.  #####################' % (i, loss_sum/step,epoch_time))\n",
    "            loss_list.append(loss_sum/step)\n",
    "            loss_sum = 0\n",
    "        \n",
    "        \n",
    "        #done with training\n",
    "        final_embeddings = normalized_embeddings.eval(session=sess)\n",
    "        sess.close()\n",
    "        #final_subgraph_embeddings = self.subgraph_embeddings.eval()\n",
    "        plt.plot(loss_list)\n",
    "        \n",
    "        return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
