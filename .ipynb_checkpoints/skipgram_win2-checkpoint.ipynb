{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ipsita/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math,logging\n",
    "from pprint import  pprint\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipgram(object):\n",
    "    '''\n",
    "    skipgram model - refer Mikolov et al (2013)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_graphs, num_subgraphs, learning_rate, embedding_size, num_negsample, num_steps, corpus):\n",
    "        \n",
    "        self.num_graphs     = num_graphs\n",
    "        self.num_subgraphs  = num_subgraphs\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_negsample  = num_negsample\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.num_steps      = num_steps\n",
    "        self.corpus         = corpus\n",
    "        self.graph, self.batch_inputs, self.batch_labels, self.normalized_embeddings,\\\n",
    "        self.loss, self.optimizer = self.trainer_initial()\n",
    "\n",
    "\n",
    "    def trainer_initial(self):\n",
    "        \n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            \n",
    "            batch_inputs = tf.placeholder(tf.int32, shape=([None, 5]))\n",
    "            batch_labels = tf.placeholder(tf.int64, shape=([None, 1]))\n",
    "\n",
    "            graph_embeddings = tf.Variable(\n",
    "                    tf.random_uniform([self.num_graphs, self.embedding_size], -0.5 / self.embedding_size, 0.5/self.embedding_size))\n",
    "\n",
    "            subgraph_embeddings = tf.Variable(\n",
    "                    tf.random_uniform([self.num_subgraphs, self.embedding_size], -0.5/self.embedding_size, 0.5/self.embedding_size))\n",
    "            #batch_graph_embeddings = tf.nn.embedding_lookup(graph_embeddings, batch_inputs) ######hidden layer\n",
    "            \n",
    "            #lookup for cbow\n",
    "            batch_graph_embeddings = tf.zeros([128, self.embedding_size])    #128 is batch_size\n",
    "            #add the doc vector\n",
    "            batch_graph_embeddings = tf.nn.embedding_lookup(graph_embeddings, batch_inputs[:,0])\n",
    "            #add window word vectors\n",
    "            for j in range(1, 5):\n",
    "                batch_graph_embeddings += tf.nn.embedding_lookup(subgraph_embeddings, batch_inputs[:, j])\n",
    "                \n",
    "            weights = tf.Variable(tf.truncated_normal([self.num_subgraphs, self.embedding_size],\n",
    "                                                          stddev=1.0 / math.sqrt(self.embedding_size))) #output layer wt\n",
    "            biases = tf.Variable(tf.zeros(self.num_subgraphs)) #output layer biases\n",
    "            \n",
    "            #negative sampling part\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.nce_loss(weights = weights,\n",
    "                               biases  = biases,\n",
    "                               labels  = batch_labels,\n",
    "                               inputs   = batch_graph_embeddings,\n",
    "                               num_sampled = self.num_negsample,\n",
    "                               num_classes = self.num_subgraphs,\n",
    "                               sampled_values = tf.nn.fixed_unigram_candidate_sampler(\n",
    "                                   true_classes = batch_labels,\n",
    "                                   num_true = 1,\n",
    "                                   num_sampled = self.num_negsample,\n",
    "                                   unique = True,\n",
    "                                   range_max = self.num_subgraphs,\n",
    "                                   distortion = 0.75,\n",
    "                                   unigrams = self.corpus.subgraph_id_freq_map_as_list)#word_id_freq_map_as_list is the\n",
    "                               # frequency of each word in vocabulary\n",
    "                               ))\n",
    "\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            learning_rate = tf.train.exponential_decay(self.learning_rate,\n",
    "                                                       global_step, 100000, 0.96, staircase=True) #linear decay over time\n",
    "\n",
    "            learning_rate = tf.maximum(learning_rate,0.001) #cannot go below 0.001 to ensure at least a minimal learning\n",
    "\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "\n",
    "            norm = tf.sqrt(tf.reduce_mean(tf.square(graph_embeddings), 1, keep_dims=True))\n",
    "            normalized_embeddings = graph_embeddings/norm\n",
    "\n",
    "        return graph, batch_inputs, batch_labels, normalized_embeddings, loss, optimizer\n",
    "\n",
    "\n",
    "\n",
    "    def train(self,corpus,batch_size):\n",
    "        \n",
    "        with tf.Session(graph=self.graph,\n",
    "                        config=tf.ConfigProto(log_device_placement=True,allow_soft_placement=False)) as sess:\n",
    "\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            for i in range(self.num_steps):\n",
    "                t0 = time.time()\n",
    "                step = 0\n",
    "                while corpus.epoch_flag == False:\n",
    "                    batch_data, batch_labels = corpus.generate_batch_from_file(batch_size)# get (target,context) word_id tuples\n",
    "\n",
    "                    feed_dict = {self.batch_inputs:batch_data,self.batch_labels:batch_labels}\n",
    "                    _,loss_val = sess.run([self.optimizer,self.loss],feed_dict=feed_dict)\n",
    "\n",
    "                    loss += loss_val\n",
    "\n",
    "                    if step % 100 == 0:\n",
    "                        if step > 0:\n",
    "                            average_loss = loss/step\n",
    "                            logging.info( 'Epoch: %d : Average loss for step: %d : %f'%(i,step,average_loss))\n",
    "                    step += 1\n",
    "\n",
    "                corpus.epoch_flag = False\n",
    "                epoch_time = time.time() - t0\n",
    "                logging.info('#########################   Epoch: %d :  %f, %.2f sec.  #####################' % (i, loss/step,epoch_time))\n",
    "                loss = 0\n",
    "\n",
    "            #done with training\n",
    "            final_embeddings = self.normalized_embeddings.eval()\n",
    "        return final_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
